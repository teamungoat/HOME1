{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1mSGdrrS1Me52nRuHP1KlwdaAZwDU6dPo","authorship_tag":"ABX9TyOL5BLn+X8bEMuStnVIxAEO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"43725599e7eb4543b5983e1b13106711":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2080d5e8d88f4f18a73760297836a6dd","IPY_MODEL_4e4e87eb8f1548f5a1d20df3e091d6a2","IPY_MODEL_1b87de011457494ba445681c5c4c81ee"],"layout":"IPY_MODEL_e33846b6476a4961bd8f63b2b1696b44"}},"2080d5e8d88f4f18a73760297836a6dd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1179f26f524437e8d8f2eab7e715058","placeholder":"​","style":"IPY_MODEL_ccc0b852deb846438366105420eadff5","value":"tokenizer.json: 100%"}},"4e4e87eb8f1548f5a1d20df3e091d6a2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c17d988bf39d4173b3e368340228da92","max":2825034,"min":0,"orientation":"horizontal","style":"IPY_MODEL_27ed28360e274dd1ab41b45a814e49e6","value":2825034}},"1b87de011457494ba445681c5c4c81ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e91de81f548147fcbc9f0a2eba652223","placeholder":"​","style":"IPY_MODEL_2b351c50c45243c9a9c1c7158fb6ca98","value":" 2.83M/2.83M [00:00&lt;00:00, 14.5MB/s]"}},"e33846b6476a4961bd8f63b2b1696b44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1179f26f524437e8d8f2eab7e715058":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccc0b852deb846438366105420eadff5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c17d988bf39d4173b3e368340228da92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27ed28360e274dd1ab41b45a814e49e6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e91de81f548147fcbc9f0a2eba652223":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b351c50c45243c9a9c1c7158fb6ca98":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"13d410a2f83343e48ae793a50bf0b01c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_feee892fdf764a9daa73fb987fd93599","IPY_MODEL_e2482f46cdc84b519d7bb00a7e8ae0f4","IPY_MODEL_4188ebd422594920be4a5a14ee8d311a"],"layout":"IPY_MODEL_0174aa6ec78841b1aeab3c1b9a91b55c"}},"feee892fdf764a9daa73fb987fd93599":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_22ed37cbfda24a9683b38965f1970719","placeholder":"​","style":"IPY_MODEL_816a3558463d466aa378805d9b7c6639","value":"config.json: 100%"}},"e2482f46cdc84b519d7bb00a7e8ae0f4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_af7cacded5e24d01937b53d8ce3d21ea","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0624f11429d34388b7ae1c0542c7416d","value":1000}},"4188ebd422594920be4a5a14ee8d311a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9edd026908944916938229200c7c8c25","placeholder":"​","style":"IPY_MODEL_4e640f18c545495fadd19ba1057969fe","value":" 1.00k/1.00k [00:00&lt;00:00, 72.6kB/s]"}},"0174aa6ec78841b1aeab3c1b9a91b55c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22ed37cbfda24a9683b38965f1970719":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"816a3558463d466aa378805d9b7c6639":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af7cacded5e24d01937b53d8ce3d21ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0624f11429d34388b7ae1c0542c7416d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9edd026908944916938229200c7c8c25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e640f18c545495fadd19ba1057969fe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1da348cd3f974c1381dc5426e0bebb39":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bd88c1f3dde548c7b4d539f0e2ec3d9d","IPY_MODEL_0344cd39a68a48ebb5d2b99b5c919acc","IPY_MODEL_c3152b6d24ba4638a7d4d032faa1097f"],"layout":"IPY_MODEL_a37ac154f4a141ed93c41c212a789577"}},"bd88c1f3dde548c7b4d539f0e2ec3d9d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2a295a0cb1e4e169c00652cd745c186","placeholder":"​","style":"IPY_MODEL_2d41fb7931a9424ab2e20e1d8c43778f","value":"pytorch_model.bin: 100%"}},"0344cd39a68a48ebb5d2b99b5c919acc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_20a83c7119f14863b3911705f2d9607b","max":513302779,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8b75bd3bf7e049bf893c28987b0d30d4","value":513302779}},"c3152b6d24ba4638a7d4d032faa1097f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_adef5c24f0d748e0b778e113a71eac4c","placeholder":"​","style":"IPY_MODEL_f7b3d4f01dd84147b368560d4c84d2b1","value":" 513M/513M [00:02&lt;00:00, 188MB/s]"}},"a37ac154f4a141ed93c41c212a789577":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2a295a0cb1e4e169c00652cd745c186":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d41fb7931a9424ab2e20e1d8c43778f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"20a83c7119f14863b3911705f2d9607b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b75bd3bf7e049bf893c28987b0d30d4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"adef5c24f0d748e0b778e113a71eac4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7b3d4f01dd84147b368560d4c84d2b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"QZdJAqfXmNbB"},"outputs":[],"source":["import math\n","import numpy as np\n","import pandas as pd\n","import random\n","import re\n","import torch\n","import urllib.request\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import PreTrainedTokenizerFast\n","import urllib.request\n","import zipfile\n","import os\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"_S3haCZzHTiY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import zipfile"],"metadata":{"id":"TP6m-OYvy4zZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["folder ='/content/drive/MyDrive/01.데이터/2.Validation/원천데이터'"],"metadata":{"id":"pvUN3Wn8uKh_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def unzip_all_files_in_folder(folder_path, extract_to_base):\n","    # 폴더 내의 모든 파일들을 순회\n","    for file_name in os.listdir(folder_path):\n","        # 파일의 전체 경로를 생성\n","        file_path = os.path.join(folder_path, file_name)\n","\n","        # ZIP 파일인지 확인\n","        if zipfile.is_zipfile(file_path):\n","            # ZIP 파일의 이름을 따서 추출할 디렉토리를 생성\n","            extract_to = os.path.join(extract_to_base, os.path.splitext(file_name)[0])\n","\n","            # 디렉토리가 존재하지 않으면 생성\n","            if not os.path.exists(extract_to):\n","                os.makedirs(extract_to)\n","\n","            # ZIP 파일을 읽고 압축 해제\n","            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n","                zip_ref.extractall(extract_to)\n","                print(f'{file_name} 파일이 {extract_to} 경로로 성공적으로 추출되었습니다.')\n","\n","# 사용 예제\n","folder_path = '/content/drive/MyDrive/01.데이터/2.Validation/원천데이터'  # ZIP 파일들이 있는 폴더 경로\n","extract_to_base = '/content/drive/MyDrive/01.데이터/2.Validation/원천데이터'  # 파일을 추출할 기본 디렉토리 경로\n","\n","unzip_all_files_in_folder(folder_path, extract_to_base)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"is9Of-oHx3J0","executionInfo":{"status":"ok","timestamp":1720416554601,"user_tz":-540,"elapsed":140234,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"89e0a0d0-c0f2-47a2-95dc-bc8b2730002a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["VS_02. FACEBOOK.zip 파일이 /content/drive/MyDrive/01.데이터/2.Validation/원천데이터/VS_02. FACEBOOK 경로로 성공적으로 추출되었습니다.\n","VS_04. BAND.zip 파일이 /content/drive/MyDrive/01.데이터/2.Validation/원천데이터/VS_04. BAND 경로로 성공적으로 추출되었습니다.\n","VS_03. INSTAGRAM.zip 파일이 /content/drive/MyDrive/01.데이터/2.Validation/원천데이터/VS_03. INSTAGRAM 경로로 성공적으로 추출되었습니다.\n","VS_05. NATEON.zip 파일이 /content/drive/MyDrive/01.데이터/2.Validation/원천데이터/VS_05. NATEON 경로로 성공적으로 추출되었습니다.\n","VS_01. KAKAO.zip 파일이 /content/drive/MyDrive/01.데이터/2.Validation/원천데이터/VS_01. KAKAO 경로로 성공적으로 추출되었습니다.\n"]}]},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"g4fWuZQp10I2","executionInfo":{"status":"ok","timestamp":1720417253049,"user_tz":-540,"elapsed":16330,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"bf36c54b-e3cd-453e-f2e2-32c40e77ed6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Collecting pyarrow>=15.0.0 (from datasets)\n","  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Collecting requests>=2.32.2 (from datasets)\n","  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.31.0\n","    Uninstalling requests-2.31.0:\n","      Successfully uninstalled requests-2.31.0\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 14.0.2\n","    Uninstalling pyarrow-14.0.2:\n","      Successfully uninstalled pyarrow-14.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-16.1.0 requests-2.32.3 xxhash-3.4.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pyarrow","requests"]},"id":"e9db80f66c0c49ef9626f65de8983ab5"}},"metadata":{}}]},{"cell_type":"code","source":["import os\n","from datasets import Dataset\n","from tqdm import tqdm\n","import time\n","from datasets import load_dataset\n","import pandas as pd"],"metadata":{"id":"_54yHs3i1Wbl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["folder=['/content/drive/MyDrive/01.데이터/2.Validation/원천데이터/VS_01. KAKAO',\n","        '/content/drive/MyDrive/01.데이터/2.Validation/원천데이터/VS_02. FACEBOOK',\n","        '/content/drive/MyDrive/01.데이터/2.Validation/원천데이터/VS_03. INSTAGRAM',\n","        '/content/drive/MyDrive/01.데이터/2.Validation/원천데이터/VS_04. BAND',\n","        '/content/drive/MyDrive/01.데이터/2.Validation/원천데이터/VS_05. NATEON']"],"metadata":{"id":"bMq4vt_604bk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = {\"Q\": [], \"A\": [], \"id\": []}"],"metadata":{"id":"lMkRba-K2VcS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["id_counter = 1\n","for folder_path in folder:\n","    for file in tqdm(os.listdir(folder_path)):\n","        if file.endswith(\".txt\"):\n","            file_path = os.path.join(folder_path, file)\n","            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","                lines = f.readlines()\n","                prev_line = \"\"\n","                prev_label = None\n","                for line in lines:\n","                    line = line.strip()\n","                    if line:\n","                        try:\n","                            label = int(line.split(\":\")[0])\n","                            text = line[len(str(label)) + 3 :].strip()\n","                            if prev_line == \"\":\n","                                prev_line = text\n","                            else:\n","                                data[\"id\"].append(id_counter)\n","                                data[\"Q\"].append(prev_line)\n","                                data[\"A\"].append(text)\n","                                id_counter += 1\n","                                prev_line = \"\"\n","                        except ValueError:\n","                            continue\n","\n","\n","# 데이터셋 생성\n","dataset = Dataset.from_dict(data)\n","\n","# 데이터셋을 CSV 파일로 저장\n","df = pd.DataFrame(data)\n","csv_filename = \"makedata.csv\"\n","df.to_csv(csv_filename, index=False, encoding=\"utf-8\")\n","\n","print(dataset[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uH8QYJzyv0mv","executionInfo":{"status":"ok","timestamp":1720417558100,"user_tz":-540,"elapsed":266961,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"4e2f56ab-e7b1-4fd9-bcc2-1bb4a9179bbd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 7605/7605 [03:16<00:00, 38.65it/s] \n","100%|██████████| 1000/1000 [00:18<00:00, 55.52it/s]\n","100%|██████████| 600/600 [00:09<00:00, 63.92it/s] \n","100%|██████████| 204/204 [00:02<00:00, 69.00it/s] \n","100%|██████████| 201/201 [00:02<00:00, 90.32it/s] \n"]},{"output_type":"stream","name":"stdout","text":["{'Q': '오늘 저녁에 오랜만에 영화 고고?', 'A': '오우 너무 좋지? ㅎ', 'id': 1}\n"]}]},{"cell_type":"code","source":["!pip install pytorch_lightning"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CBLOjiiG4gyf","executionInfo":{"status":"ok","timestamp":1720417734861,"user_tz":-540,"elapsed":77219,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"2b46273a-3aaa-4ce1-e747-cf1c3596a98a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch_lightning\n","  Downloading pytorch_lightning-2.3.2-py3-none-any.whl (812 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.3/812.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.25.2)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.3.0+cu121)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.4)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.1)\n","Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2023.6.0)\n","Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n","  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (24.1)\n","Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.12.2)\n","Collecting lightning-utilities>=0.10.0 (from pytorch_lightning)\n","  Downloading lightning_utilities-0.11.3.post0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2.32.3)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.9.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (67.7.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (3.15.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.0.0->pytorch_lightning)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.0.0->pytorch_lightning)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.0.0->pytorch_lightning)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.0.0->pytorch_lightning)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.0.0->pytorch_lightning)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.0.0->pytorch_lightning)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.0.0->pytorch_lightning)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.0.0->pytorch_lightning)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.0.0->pytorch_lightning)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.0.0->pytorch_lightning)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.0.0->pytorch_lightning)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->pytorch_lightning)\n","  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->pytorch_lightning) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2024.6.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->pytorch_lightning) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch_lightning\n","Successfully installed lightning-utilities-0.11.3.post0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 pytorch_lightning-2.3.2 torchmetrics-1.4.0.post0\n"]}]},{"cell_type":"code","source":["from pytorch_lightning import LightningModule"],"metadata":{"id":"oWfHL8Lk5fLQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import torch\n","from pytorch_lightning import Trainer\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","\n","from torch.utils.data import DataLoader, Dataset\n","from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n","from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n","import re, os\n","from tqdm import tqdm\n","\n","\n","Q_TKN = \"<usr>\"\n","A_TKN = \"<sys>\"\n","BOS = \"</s>\"\n","EOS = \"</s>\"\n","MASK = \"<unused0>\"\n","SENT = \"<unused1>\"\n","PAD = \"<pad>\"\n","\n","save_dir = \"saved_models\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","\n","print(\"start1\")\n","\n","\n","class ChatbotDataset(Dataset):\n","    def __init__(self, chats, max_len=40):  # 데이터셋의 전처리를 해주는 부분\n","        self._data = chats\n","        self.max_len = max_len\n","        self.q_token = Q_TKN\n","        self.a_token = A_TKN\n","        self.sent_token = SENT\n","        self.eos = EOS\n","        self.mask = MASK\n","        self.tokenizer = koGPT2_TOKENIZER\n","\n","    def __len__(self):  # chatbotdata 의 길이를 리턴한다.\n","        return len(self._data)\n","\n","    def __getitem__(self, idx):  # 로드한 챗봇 데이터를 차례차례 DataLoader로 넘겨주는 메서드\n","        turn = self._data.iloc[idx]\n","        q = turn[\"Q\"]  # 질문을 가져온다.\n","        q = re.sub(r\"([?.!,])\", r\" \", q)  # 구둣점들을 제거한다.\n","\n","        a = turn[\"A\"]  # 답변을 가져온다.\n","        a = re.sub(r\"([?.!,])\", r\" \", a)  # 구둣점들을 제거한다.\n","\n","        q_toked = self.tokenizer.tokenize(self.q_token + q + self.sent_token)\n","        q_len = len(q_toked)\n","\n","        a_toked = self.tokenizer.tokenize(self.a_token + a + self.eos)\n","        a_len = len(a_toked)\n","\n","        # 질문의 길이가 최대길이보다 크면\n","        if q_len > self.max_len:\n","            a_len = self.max_len - q_len  # 답변의 길이를 최대길이 - 질문길이\n","            if a_len <= 0:  # 질문의 길이가 너무 길어 질문만으로 최대 길이를 초과 한다면\n","                q_toked = q_toked[-(int(self.max_len / 2)) :]  # 질문길이를 최대길이의 반으로\n","                q_len = len(q_toked)\n","                a_len = self.max_len - q_len  # 답변의 길이를 최대길이 - 질문길이\n","            a_toked = a_toked[:a_len]\n","            a_len = len(a_toked)\n","\n","        # 질문의 길이 + 답변의 길이가 최대길이보다 크면\n","        if q_len + a_len > self.max_len:\n","            a_len = self.max_len - q_len  # 답변의 길이를 최대길이 - 질문길이\n","            if a_len <= 0:  # 질문의 길이가 너무 길어 질문만으로 최대 길이를 초과 한다면\n","                q_toked = q_toked[-(int(self.max_len / 2)) :]  # 질문길이를 최대길이의 반으로\n","                q_len = len(q_toked)\n","                a_len = self.max_len - q_len  # 답변의 길이를 최대길이 - 질문길이\n","            a_toked = a_toked[:a_len]\n","            a_len = len(a_toked)\n","\n","        # 답변 labels = [mask, mask, ...., mask, ..., <bos>,..답변.. <eos>, <pad>....]\n","        labels = [\n","            self.mask,\n","        ] * q_len + a_toked[1:]\n","\n","        # mask = 질문길이 0 + 답변길이 1 + 나머지 0\n","        mask = [0] * q_len + [1] * a_len + [0] * (self.max_len - q_len - a_len)\n","        # 답변 labels을 index 로 만든다.\n","        labels_ids = self.tokenizer.convert_tokens_to_ids(labels)\n","        # 최대길이만큼 PADDING\n","        while len(labels_ids) < self.max_len:\n","            labels_ids += [self.tokenizer.pad_token_id]\n","\n","        # 질문 + 답변을 index 로 만든다.\n","        token_ids = self.tokenizer.convert_tokens_to_ids(q_toked + a_toked)\n","        # 최대길이만큼 PADDING\n","        while len(token_ids) < self.max_len:\n","            token_ids += [self.tokenizer.pad_token_id]\n","\n","        # 질문+답변, 마스크, 답변\n","        return (token_ids, np.array(mask), labels_ids)\n","\n","\n","def collate_batch(batch):\n","    data = [item[0] for item in batch]\n","    mask = [item[1] for item in batch]\n","    label = [item[2] for item in batch]\n","    return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)\n","\n","\n","koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained(\n","    \"skt/kogpt2-base-v2\",\n","    bos_token=BOS,\n","    eos_token=EOS,\n","    unk_token=\"<unk>\",\n","    pad_token=PAD,\n","    mask_token=MASK,\n",")\n","model = GPT2LMHeadModel.from_pretrained(\"skt/kogpt2-base-v2\")\n","\n","dataname = \"/content/makedata.csv\"  # Adjusted file path\n","Chatbot_Data = pd.read_csv(dataname)\n","Chatbot_Data.dropna(subset=[\"A\"], inplace=True)\n","Chatbot_Data.dropna(subset=[\"Q\"], inplace=True)\n","Chatbot_Data.dropna(subset=[\"id\"], inplace=True)\n","\n","\n","print(\"start3\")\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","train_set = ChatbotDataset(Chatbot_Data, max_len=40)\n","train_dataloader = DataLoader(\n","    train_set,\n","    batch_size=32,\n","    num_workers=0,\n","    shuffle=True,\n","    collate_fn=collate_batch,\n",")\n","\n","model.to(device)\n","\n","learning_rate = 3e-5\n","criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","num_epochs = 10\n","Sneg = -1e18\n","\n","for epoch in range(num_epochs):\n","    dataloader = tqdm(train_dataloader, desc=f\"Epoch {epoch}\")\n","    for batch_idx, samples in enumerate(dataloader):\n","        optimizer.zero_grad()\n","        token_ids, mask, label = samples\n","        token_ids, mask, label = token_ids.to(device), mask.to(device), label.to(device)\n","        out = model(token_ids)\n","        out = out.logits\n","        mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2], dim=2)\n","        mask_out = torch.where(mask_3d == 1, out, Sneg * torch.ones_like(out))\n","        loss = criterion(mask_out.transpose(2, 1), label)\n","        avg_loss = loss.sum() / mask.sum()\n","        avg_loss.backward()\n","        optimizer.step()\n","\n","model_save_path = os.path.join(save_dir, \"chatbot_model.pth\")\n","torch.save(\n","    {\n","        \"model_state_dict\": model.state_dict(),\n","        \"optimizer_state_dict\": optimizer.state_dict(),\n","        \"epoch\": epoch,\n","    },\n","    model_save_path,\n",")\n","\n","print(\"Model saved at:\", model_save_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4AXMtFpD3XZq","executionInfo":{"status":"ok","timestamp":1720428965049,"user_tz":-540,"elapsed":9989999,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"5f14cdcb-ad87-4eeb-93b8-002a975b199a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["start1\n"]},{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n","The class this function is called from is 'PreTrainedTokenizerFast'.\n"]},{"output_type":"stream","name":"stdout","text":["start3\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 0:   0%|          | 0/2833 [00:00<?, ?it/s]<ipython-input-16-52ae4a8ca6d8>:104: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n","  return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)\n","We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n","Epoch 0: 100%|██████████| 2833/2833 [16:33<00:00,  2.85it/s]\n","Epoch 1: 100%|██████████| 2833/2833 [16:37<00:00,  2.84it/s]\n","Epoch 2: 100%|██████████| 2833/2833 [16:37<00:00,  2.84it/s]\n","Epoch 3: 100%|██████████| 2833/2833 [16:37<00:00,  2.84it/s]\n","Epoch 4: 100%|██████████| 2833/2833 [16:37<00:00,  2.84it/s]\n","Epoch 5: 100%|██████████| 2833/2833 [16:38<00:00,  2.84it/s]\n","Epoch 6: 100%|██████████| 2833/2833 [16:37<00:00,  2.84it/s]\n","Epoch 7: 100%|██████████| 2833/2833 [16:38<00:00,  2.84it/s]\n","Epoch 8: 100%|██████████| 2833/2833 [16:38<00:00,  2.84it/s]\n","Epoch 9: 100%|██████████| 2833/2833 [16:38<00:00,  2.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Model saved at: saved_models/chatbot_model.pth\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n","\n","# Define your special tokens and tokenizer\n","Q_TKN = \"<usr>\"\n","A_TKN = \"<sys>\"\n","SENT = \"<unused1>\"\n","EOS = \"</s>\"\n","BOS = \"</s>\"\n","\n","# Initialize your tokenizer\n","koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained(\n","    \"skt/kogpt2-base-v2\",\n","    bos_token=BOS,\n","    eos_token=EOS,\n","    unk_token=\"<unk>\",\n","    pad_token=\"<pad>\",\n","    mask_token=\"<unused0>\",\n",")\n","\n","# Load your trained model\n","model_path = \"/content/drive/MyDrive/model/chatbot_model.pth\"  # Adjust the path to your saved model\n","model = GPT2LMHeadModel.from_pretrained(\"skt/kogpt2-base-v2\")\n","checkpoint = torch.load(model_path)\n","model.load_state_dict(checkpoint[\"model_state_dict\"])\n","model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":739,"referenced_widgets":["43725599e7eb4543b5983e1b13106711","2080d5e8d88f4f18a73760297836a6dd","4e4e87eb8f1548f5a1d20df3e091d6a2","1b87de011457494ba445681c5c4c81ee","e33846b6476a4961bd8f63b2b1696b44","a1179f26f524437e8d8f2eab7e715058","ccc0b852deb846438366105420eadff5","c17d988bf39d4173b3e368340228da92","27ed28360e274dd1ab41b45a814e49e6","e91de81f548147fcbc9f0a2eba652223","2b351c50c45243c9a9c1c7158fb6ca98","13d410a2f83343e48ae793a50bf0b01c","feee892fdf764a9daa73fb987fd93599","e2482f46cdc84b519d7bb00a7e8ae0f4","4188ebd422594920be4a5a14ee8d311a","0174aa6ec78841b1aeab3c1b9a91b55c","22ed37cbfda24a9683b38965f1970719","816a3558463d466aa378805d9b7c6639","af7cacded5e24d01937b53d8ce3d21ea","0624f11429d34388b7ae1c0542c7416d","9edd026908944916938229200c7c8c25","4e640f18c545495fadd19ba1057969fe","1da348cd3f974c1381dc5426e0bebb39","bd88c1f3dde548c7b4d539f0e2ec3d9d","0344cd39a68a48ebb5d2b99b5c919acc","c3152b6d24ba4638a7d4d032faa1097f","a37ac154f4a141ed93c41c212a789577","d2a295a0cb1e4e169c00652cd745c186","2d41fb7931a9424ab2e20e1d8c43778f","20a83c7119f14863b3911705f2d9607b","8b75bd3bf7e049bf893c28987b0d30d4","adef5c24f0d748e0b778e113a71eac4c","f7b3d4f01dd84147b368560d4c84d2b1"]},"id":"jwffEEwq2CQa","executionInfo":{"status":"ok","timestamp":1721194977885,"user_tz":-540,"elapsed":24123,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"1d5a762e-3856-4b8d-e390-bbf858956063"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/2.83M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43725599e7eb4543b5983e1b13106711"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13d410a2f83343e48ae793a50bf0b01c"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n","The class this function is called from is 'PreTrainedTokenizerFast'.\n"]},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1da348cd3f974c1381dc5426e0bebb39"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(51200, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",")"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Interaction loop with the chatbot 기존 학습 모델은 model, tokenizer 반말을 존댓말로 바꾸는 모델은 model1, tokenizer1\n","with torch.no_grad():\n","    while 1:\n","        q = input(\"user > \").strip()\n","        if q == \"quit\":\n","            break\n","        a = \"\"\n","        while 1:\n","            input_ids = torch.LongTensor(\n","                koGPT2_TOKENIZER.encode(Q_TKN + q + SENT + A_TKN + a)\n","            ).unsqueeze(dim=0)\n","            pred = model(input_ids)\n","            pred = pred.logits\n","            gen = koGPT2_TOKENIZER.convert_ids_to_tokens(\n","                torch.argmax(pred, dim=-1).squeeze().numpy().tolist()\n","            )[-1]\n","            if gen == EOS:\n","                break\n","            a += gen.replace(\"▁\", \" \")\n","            input_text= a #a는 챗봇 모델이 생성하는 응답\n","            input_encoding = tokenizer1(input_text, return_tensors=\"pt\")\n","            input_ids1 = input_encoding.input_ids.to(device)\n","            attention_mask1 = input_encoding.attention_mask.to(device)\n","\n","            # T5 모델 출력 생성\n","            output_encoding = model1.generate(\n","                input_ids=input_ids1,\n","                attention_mask=attention_mask1,\n","                max_length=128,\n","                num_beams=5,\n","                early_stopping=True,\n","            )\n","            output_text = tokenizer1.decode(output_encoding[0], skip_special_tokens=True)\n","        print(\"Chatbot > {}\".format(output_text.strip())) #output_text는 챗봇이 응답한 변수인 a를 존댓말로 변환한 말"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":573},"id":"uRWpVjHSI3tZ","executionInfo":{"status":"error","timestamp":1721195931582,"user_tz":-540,"elapsed":228995,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"d491bdf8-9644-4efb-8469-64f469c9c1cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["user > 머했어?\n","Chatbot > 아니요 지금 살고 있는 곳이 머한데.\n","user > 어제 영화 머봤어?\n","Chatbot > 아니요 그거 너무 잼나는데.\n","user > 밥은 먹었니?\n","Chatbot > 오늘 점심 어떤 거 드셨어요.\n","user > 영화 추천 해줘\n","Chatbot > 나는 모가디슈가 젤 재밌어요.\n","user > 얌마 말좀 제대로 해봐\n","Chatbot > 네 저 진짜 편해서 너가 저희 가자고 하면 바로 달려갈 듯해요.\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-33fab54672bd>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user > \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]},{"cell_type":"code","source":["import torch\n","from transformers import T5ForConditionalGeneration, T5Tokenizer\n","\n","# T5 모델 로드\n","model1 = T5ForConditionalGeneration.from_pretrained(\"j5ng/et5-formal-convertor\")\n","tokenizer1 = T5Tokenizer.from_pretrained(\"j5ng/et5-formal-convertor\")\n","\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","# device = \"mps:0\" if torch.cuda.is_available() else \"cpu\" # for mac m1\n","\n","model1 = model1.to(device) # 모델을 GPU로 이동"],"metadata":{"id":"TMqoJbBukt-a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 예시 입력 문장\n","input_text = \"나 진짜 화났어 지금\"\n","\n","# 입력 문장 인코딩\n","input_encoding = tokenizer1(\"존댓말로 바꿔주세요: \" + input_text, return_tensors=\"pt\")\n","\n","input_ids1 = input_encoding.input_ids.to(device)\n","attention_mask1 = input_encoding.attention_mask.to(device)\n","\n","# T5 모델 출력 생성\n","output_encoding = model1.generate(\n","    input_ids=input_ids1,\n","    attention_mask=attention_mask1,\n","    max_length=128,\n","    num_beams=5,\n","    early_stopping=True,\n",")\n","\n","# 출력 문장 디코딩\n","output_text = tokenizer1.decode(output_encoding[0], skip_special_tokens=True)\n","\n","# 결과 출력\n","print(output_text) # 저 진짜 화났습니다 지금."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I8dBvYR1MsjQ","executionInfo":{"status":"ok","timestamp":1721195688007,"user_tz":-540,"elapsed":810,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"0a8cda5d-cbfd-4b80-c831-abed432360ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["저 진짜 화났습니다 지금.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"-3ZanJVUQHyc"},"execution_count":null,"outputs":[]}]}